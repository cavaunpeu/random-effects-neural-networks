{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Effects Neural Networks with Edward and Keras\n",
    "Bayesian probabilistic models provide a nimble and expressive framework for modeling \"small-world\" data plus a host of tools for vigorously criticizing the models we build. In contrast, deep learning offers a more rigid yet much more powerful framework for modeling data of massive size. [Edward](http://edwardlib.org/) is a probabilistic programming library that bridges this gap: \"black-box\" variational inference enables us to fit extremely flexible Bayesian models to large-scale data. Furthermore, these models themselves may take advantage of classic deep-learning architectures of arbitrary complexity.\n",
    "\n",
    "Edward uses [TensorFlow](https://www.tensorflow.org/) for symbolic gradients and data flow graphs. As such, it interfaces cleanly with other libraries that do the same, namely [TF-Slim](https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html), [PrettyTensor](https://github.com/google/prettytensor) and [Keras](https://keras.io/). Personally, I've been working often with the latter, and am consistently delighted by the ease with which it allows me to specify complex neural architectures.\n",
    "\n",
    "The aim of this post is to lay a practical foundation for Bayesian modeling in Edward, then explore how, and how easily, we can extend these models in the direction of classical deep learning via Keras. It will give both a conceptual overview of the models below, as well as notes on the practical considerations of their implementation —  what worked and what didn't. Finally, this post will conclude with concrete ways in which to extend these models further, of which there are many.\n",
    "\n",
    "If you're just getting started with Edward or Keras, I recommend first perusing the [Edward tutorials](http://edwardlib.org/tutorials) and [Keras documentation](https://keras.io/) respectively.\n",
    "\n",
    "To \"pull us down the path,\" we build three models in additive fashion: a Bayesian linear regression model, a Bayesian linear regression model with random effects, and a neural network with random effects. We fit these models on the [Zillow Prize](https://www.kaggle.com/c/zillow-prize-1) dataset, which asks us to predict `logerror` (in house-price estimate, i.e. the \"Zestimate\") given metadata for a list of homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "from keras.layers import Input, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = ed.get_session()\n",
    "K.set_session(sess)\n",
    "\n",
    "INIT_OP = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preparation\n",
    "After importing the data we rename its columns as per Philipp Spachtholz's [Exploratory Analysis - Zillow](https://www.kaggle.com/philippsp/exploratory-analysis-zillow) kernel on [kaggle.com](https://www.kaggle.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df = pd.read_csv('data/properties.csv', low_memory=False)\n",
    "transactions_df = pd.read_csv('data/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "properties_df = properties_df.rename(columns={\n",
    "    'parcelid': 'id_parcel',\n",
    "    'yearbuilt': 'build_year',\n",
    "    'basementsqft': 'area_basement',\n",
    "    'yardbuildingsqft17': 'area_patio',\n",
    "    'yardbuildingsqft26': 'area_shed',\n",
    "    'poolsizesum': 'area_pool',\n",
    "    'lotsizesquarefeet': 'area_lot',\n",
    "    'garagetotalsqft': 'area_garage',\n",
    "    'finishedfloor1squarefeet': 'area_firstfloor_finished',\n",
    "    'calculatedfinishedsquarefeet': 'area_total_calc',\n",
    "    'finishedsquarefeet6': 'area_base',\n",
    "    'finishedsquarefeet12': 'area_live_finished',\n",
    "    'finishedsquarefeet13': 'area_liveperi_finished',\n",
    "    'finishedsquarefeet15': 'area_total_finished',\n",
    "    'finishedsquarefeet50': 'area_unknown',\n",
    "    'unitcnt': 'num_unit',\n",
    "    'numberofstories': 'num_story',\n",
    "    'roomcnt': 'num_room',\n",
    "    'bathroomcnt': 'num_bathroom',\n",
    "    'bedroomcnt': 'num_bedroom',\n",
    "    'calculatedbathnbr': 'num_bathroom_calc',\n",
    "    'fullbathcnt': 'num_bath',\n",
    "    'threequarterbathnbr': 'num_75_bath',\n",
    "    'fireplacecnt': 'num_fireplace',\n",
    "    'poolcnt': 'num_pool',\n",
    "    'garagecarcnt': 'num_garage',\n",
    "    'regionidcounty': 'region_county',\n",
    "    'regionidcity': 'region_city',\n",
    "    'regionidzip': 'region_zip',\n",
    "    'regionidneighborhood': 'region_neighbor',\n",
    "    'taxvaluedollarcnt': 'tax_total',\n",
    "    'structuretaxvaluedollarcnt': 'tax_building',\n",
    "    'landtaxvaluedollarcnt': 'tax_land',\n",
    "    'taxamount': 'tax_property',\n",
    "    'assessmentyear': 'tax_year',\n",
    "    'taxdelinquencyflag': 'tax_delinquency',\n",
    "    'taxdelinquencyyear': 'tax_delinquency_year',\n",
    "    'propertyzoningdesc': 'zoning_property',\n",
    "    'propertylandusetypeid': 'zoning_landuse',\n",
    "    'propertycountylandusecode': 'zoning_landuse_county',\n",
    "    'fireplaceflag': 'flag_fireplace',\n",
    "    'hashottuborspa': 'flag_tub',\n",
    "    'buildingqualitytypeid': 'quality',\n",
    "    'buildingclasstypeid': 'framing',\n",
    "    'typeconstructiontypeid': 'material',\n",
    "    'decktypeid': 'deck',\n",
    "    'storytypeid': 'story',\n",
    "    'heatingorsystemtypeid': 'heating',\n",
    "    'airconditioningtypeid': 'aircon',\n",
    "    'architecturalstyletypeid': 'architectural_style'\n",
    "})\n",
    "\n",
    "transactions_df = transactions_df.rename(columns={\n",
    "  'parcelid': 'id_parcel',\n",
    "  'transactiondate': 'date'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = transactions_df.merge(properties_df, how='left', left_on='id_parcel', right_on='id_parcel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns containing too many nulls\n",
    "Bayesian probabilistic models allow us to flexibly model *missing* data itself. To this end, we conceive of a given predictor as a vector of both:\n",
    "    1. Observed values.\n",
    "    2. Parameters in place of missing values, which will form a posterior distribution for what this value might have been.\n",
    "    \n",
    "In a (partially-specified, for brevity) linear model, this might look as follows:\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\mu_i, \\sigma)\\\\\n",
    "\\mu_i = \\alpha + \\beta_N N_i\\\\\n",
    "N_i \\sim \\mathcal{N}(\\nu, \\sigma_N)\\\\\n",
    "$$\n",
    "\n",
    "where $N_i$ is our sometimes-missing predictor. When $N_i$ is observed, $\\mathcal{N}(\\nu, \\sigma_N)$ serves as a likelihood: given this data-point, we tweak retrodictive distributions on the parameters $(\\nu, \\sigma_N)$ by which it was produced. Conversely, when $N_i$ is missing it serves as a prior: after learning distributions of $(\\nu, \\sigma_N)$ we can generate a likely value of $N_i$ itself. Finally, inference will give us (presumably wide) distributions on the model's belief in what was the true value of each missing $N_i$, conditional on the data observed.\n",
    "\n",
    "I tried this in Edward, albeit briefly, to no avail. Dustin Tran gives an [example](https://discourse.edwardlib.org/t/how-to-handle-missing-values-in-gaussian-matrix-factorization/95/2) of how one might accomplish this task in the case of Gaussian Matrix Factorization. In my case, I wasn't able to apply a 2-D missing-data-mask placeholder to a 2-D data placeholder via [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) nor [`tf.gather_nd`](https://www.tensorflow.org/api_docs/python/tf/gather_nd). With more effort, I'm sure I could get this to work. Help appreciated.\n",
    "\n",
    "For now, we'll first drop columns containing too many null values, then, after choosing a few of the predictors most correlated to the target, drop the remaining rows containing nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_cols = data.columns[ data.isnull().mean() < .25 ]\n",
    "data = data[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which columns are most correlated with the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logerror               1.000000\n",
       "area_live_finished     0.041922\n",
       "area_total_calc        0.038784\n",
       "num_bathroom_calc      0.029448\n",
       "num_bath               0.028845\n",
       "num_bathroom           0.027889\n",
       "num_bedroom            0.025467\n",
       "tax_building           0.022085\n",
       "build_year             0.017312\n",
       "censustractandblock    0.008892\n",
       "Name: logerror, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_cols = [col for col in data.columns if data[col].dtype == np.float64]\n",
    "\n",
    "data[float_cols]\\\n",
    "    .corr()['logerror']\\\n",
    "    .abs()\\\n",
    "    .sort_values(ascending=False)\\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select three fixed-effect predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_effect_predictors = [\n",
    "    'area_live_finished', \n",
    "    'num_bathroom', \n",
    "    'build_year'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select one random-effect predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_codes = data['region_zip'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "    X_train:  (36986, 3)\n",
      "    X_val:    (36986, 3)\n",
      "    y_train:  (36986,)\n",
      "    y_val:    (36986,)\n"
     ]
    }
   ],
   "source": [
    "train_index = data.sample(frac=0.5).index\n",
    "val_index = data.drop(train_index).index\n",
    "\n",
    "X = data.drop('logerror', axis=1)[fixed_effect_predictors]\n",
    "X = scale(X)\n",
    "y = data['logerror'].values\n",
    "\n",
    "X_train = X[train_index]\n",
    "y_train = y[train_index]\n",
    "X_val = X[val_index]\n",
    "y_val = y[val_index]\n",
    "\n",
    "print('Dataset sizes:')\n",
    "print(f'    X_train:  {X_train.shape}')\n",
    "print(f'    X_val:    {X_val.shape}')\n",
    "print(f'    y_train:  {y_train.shape}')\n",
    "print(f'    y_val:    {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian linear regression\n",
    "Using three fixed-effect predictors we'll fit a model of the following form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D = X_train.shape\n",
    "\n",
    "# fixed-effects placeholders\n",
    "fixed_effects = tf.placeholder(tf.float32, [N, D])\n",
    "\n",
    "# fixed-effects parameters\n",
    "β_fixed_effects = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "α = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "\n",
    "# model\n",
    "μ_y = α + ed.dot(fixed_effects, β_fixed_effects)\n",
    "y = Normal(loc=μ_y, scale=tf.ones(N))\n",
    "\n",
    "# approximate fixed-effects distributions \n",
    "qβ_fixed_effects = Normal(\n",
    "    loc=tf.Variable(tf.random_normal([D])),\n",
    "    scale=tf.nn.softplus(tf.Variable(tf.random_normal([D])))\n",
    ")\n",
    "qα = Normal(\n",
    "    loc=tf.Variable(tf.random_normal([1])),\n",
    "    scale=tf.nn.softplus(tf.Variable(tf.random_normal([1])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = {\n",
    "    β_fixed_effects: qβ_fixed_effects,\n",
    "    α: qα\n",
    "}\n",
    "\n",
    "sess.run(INIT_OP)\n",
    "inference = ed.KLqp(latent_vars, data={fixed_effects: X_train, y: y_train})\n",
    "inference.run(n_samples=5, n_iter=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# something about what the softplus is, why we do the following, etc.\n",
    "# something about variable shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_data_fit(X, y, β, α, title_prefix, n_samples=10):\n",
    "    '''Plot lines generated via samples from parameter distributions of the first \n",
    "    two fixed effects, vs. observed data points.\n",
    "    Args:\n",
    "        X (np.array) : A design matrix of fixed effects.\n",
    "        y (np.array) : A vector of responses.\n",
    "        β (ed.RandomVariable) : A (presumably) multivariate distribution of fixed-effect parameters.\n",
    "        α (ed.RandomVariable) : A univariate distribution of the model's intercept term.\n",
    "        title_prefix (str) : A string to append to the beginning of the title.\n",
    "        n_samples (int) : The number of lines to plot as drawn from the parameter distributions.\n",
    "    '''\n",
    "    \n",
    "    # draw samples from parameter distributions\n",
    "    β_samples = β.sample(n_samples).eval()\n",
    "    α_samples = α.sample(n_samples).eval()\n",
    "    \n",
    "    # plot the first two dimensions of `X`, vs. `y`\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], y)\n",
    "    plt.title(f'{title_prefix} Parameter Samples vs. Observed Data')\n",
    "    \n",
    "    # plot lines defined by parameter samples\n",
    "    inputs = np.linspace(-10, 10, num=500)\n",
    "    for i in range(n_samples):\n",
    "        output = inputs * β_samples[i][0] + inputs * β_samples[i][1] + α_samples[i][0]\n",
    "        ax.plot(inputs, inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data fit given parameter priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_fit(X_train, y_train, β_fixed_effects, α, 'Prior', n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data fit given parameter posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_data_fit(X_train, y_train, qβ_fixed_effects, qα, 'Posterior', n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note the axis scales as well!\n",
    "\n",
    "# just because we fit the data doesn't mean we're fitting it well. this will become clearer when we compute the MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Inspect residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean_absolute_error(y_posterior, X_val_feed_dict, y_val=y_val):\n",
    "    data = {y_posterior: y_val}\n",
    "    data.update(X_val_feed_dict)\n",
    "    mae = ed.evaluate('mean_absolute_error', data=data)\n",
    "    print(f'Mean absolute error on validation data: {mae:1.5}')\n",
    "    \n",
    "def plot_residuals(y_posterior, X_val_feed_dict, title, y_val=y_val):\n",
    "    y_posterior_preds = y_posterior.eval(feed_dict=X_val_feed_dict)\n",
    "    plt.hist(y_posterior_preds - y_val, edgecolor='white', linewidth=1, bins=30, alpha=.7)\n",
    "    plt.axvline(0, color='#A60628', linestyle='--')\n",
    "    plt.xlabel('`y_posterior_preds - y_val`', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.title(title, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if we omit the `.mean()` we still get point estimates for our posterior predictive. how do we get the full distribution?\n",
    "param_posteriors = {\n",
    "    β_fixed_effects: qβ_fixed_effects.mean(),\n",
    "    α: qα.mean()\n",
    "}\n",
    "X_val_feed_dict = {\n",
    "    fixed_effects: X_val\n",
    "}\n",
    "y_posterior = ed.copy(y, param_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compute_mean_absolute_error(y_posterior, X_val_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_posterior, X_val_feed_dict, title='Linear Regression Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect parameter posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw samples from posteriors\n",
    "qβ_fixed_effects_samples = qβ_fixed_effects.sample(1000).eval()\n",
    "qα_samples = qα.sample(1000).eval()\n",
    "\n",
    "# plot samples\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for dimension in range(D):\n",
    "    subplot = plt.subplot(221 + dimension)\n",
    "    plt.hist(qβ_fixed_effects_samples[:, dimension], edgecolor='white', linewidth=1, bins=30, alpha=.7)\n",
    "    plt.axvline(0, color='#A60628', linestyle='--')\n",
    "    title = f'Posterior Distribution of `{fixed_effect_predictors[dimension]}` Effect'\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    \n",
    "subplot = plt.subplot(221 + dimension + 1)\n",
    "plt.hist(qα_samples, edgecolor='white', linewidth=1, bins=30, alpha=.7)\n",
    "plt.axvline(0, color='#A60628', linestyle='--')\n",
    "title = f'Posterior Distribution of Fixed Intercept α'\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.title(title, fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"conditional on the assumptions that the log error and other variables can be related by a straight line...\"\n",
    "# \"multivariate regression model allows us to the answer the question: “what the is the predictive value of one variable once I already know the values of all other variables?”\"\n",
    "# - once I know A, what additional value is there in knowing B?\n",
    "# - once I know B, what additional value is there in knowing A?\n",
    "# the addition can be interpreted as an “or:”  μ  is a function of  x1  or  x2 \n",
    "# the “or” indicates independent associations, which may be statistical or rather causal\n",
    "\n",
    "# conservative maximum entropy distributions do an excellent job in this context\n",
    "# multilevel models also go by the following names:\n",
    "# hierarchical\n",
    "# mixed effects\n",
    "\n",
    "# intercepts were “shrunk” towards the intercept-mean  α , as the two extra parameters ( α  and  σ ) and their hyperpriors allowed the model to learn a more aggressive regularizing prior in an adaptive context\n",
    "\n",
    "# in the above multilevel tadpole model, we put a prior around the variance  σ  of the population survival proportion distribution\n",
    "# this can be problematic for two reasons:\n",
    "# if we only had 5 clusters (in our example, we had 48), that’s a bit like trying to estimate a variance with 5 data points\n",
    "# in non-linear models with logit and log links, floor and ceiling effects sometimes render extreme values of the variance equally plausible as more realistic values\n",
    "\n",
    "# in the partial pooling (multilevel) approach, we get results that walk the line between underfit and overfit\n",
    "# small clusters are prone to overfit, so they receive a bigger dose of the underfit grand mean (complete pooling)\n",
    "# large clusters “shrink” much less because they contain more information and are less prone to overfitting (and therefore need less correcting, i.e. “shrinking”), and therefore just don’t have very far to go (i.e. be pulled towards a global mean)\n",
    "# while partial pooling has little effect on large clusters, the information learned from these clusters can help substantially with prediction in small clusters\n",
    "# the whole point is to shrink estimates towards the mean\n",
    "# regularization\n",
    "\n",
    "# this “pulling/shrinking/learn things amongst the clusters” is a form of regularization, but now an amount of regularization that is learned from the data itself\n",
    "\n",
    "#   - in this way, you can think of the sigma parameter for each cluster as a crude measure of the cluster's relevance for explaining variation in the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression with random effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_zip_codes = len(set(zip_codes))\n",
    "\n",
    "# random-effect placeholder\n",
    "zip_codes_ph = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# random-effect parameter\n",
    "σ_zip_code = tf.sqrt(tf.exp(tf.Variable(tf.random_normal([]))))\n",
    "α_zip_code = Normal(loc=tf.zeros(n_zip_codes), scale=σ_zip_code * tf.ones(n_zip_codes))\n",
    "    \n",
    "# model\n",
    "α_random_effects = tf.gather(α_zip_code, zip_codes_ph)\n",
    "μ_y = α + α_random_effects + ed.dot(fixed_effects, β_fixed_effects)\n",
    "y = Normal(loc=μ_y, scale=tf.ones(N))\n",
    "\n",
    "# approximate random-effect distribution\n",
    "qα_zip_code = Normal(\n",
    "    loc=tf.Variable(tf.random_normal([n_zip_codes])),\n",
    "    scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_zip_codes])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent_vars = {\n",
    "    β_fixed_effects: qβ_fixed_effects,\n",
    "    α: qα,\n",
    "    α_zip_code: qα_zip_code\n",
    "}\n",
    "\n",
    "sess.run(INIT_OP)\n",
    "inference = ed.KLqp(latent_vars, data={fixed_effects: X_train, zip_codes_ph: zip_codes[train_index], y: y_train})\n",
    "inference.run(n_samples=5, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_posteriors = {\n",
    "    β_fixed_effects: qβ_fixed_effects.mean(),\n",
    "    α: qα.mean(),\n",
    "    α_zip_code: qα_zip_code.mean()\n",
    "}\n",
    "X_val_feed_dict = {\n",
    "    fixed_effects: X_val,\n",
    "    zip_codes_ph: zip_codes[val_index]\n",
    "}\n",
    "y_posterior = ed.copy(y, param_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mean_absolute_error(y_posterior, X_val_feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Inspect residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(y_posterior, X_val_feed_dict, title='Linear Regression with Random Effects Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Inspect shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_vars = {\n",
    "    β_fixed_effects: qβ_fixed_effects,\n",
    "    α: qα\n",
    "}\n",
    "qα_means = {}\n",
    "for zip_code in zip_codes[train_index].unique():\n",
    "    # compute mask, number of observations\n",
    "    mask = zip_codes[train_index] == zip_code\n",
    "    N_ = mask.sum()\n",
    "    \n",
    "    # instantiate model for current zip_code\n",
    "    fixed_effects = tf.placeholder(tf.float32, [N_, D])\n",
    "    μ_y = α + ed.dot(fixed_effects, β_fixed_effects)\n",
    "    y = Normal(loc=μ_y, scale=tf.ones(N_))\n",
    "    \n",
    "    # fit model\n",
    "    sess.run(INIT_OP)\n",
    "    inference = ed.KLqp(latent_vars, data={fixed_effects: X_train[mask], y: y_train[mask]})\n",
    "    inference.run(n_samples=5, n_iter=250)\n",
    "    \n",
    "    # log mean of qα for this zip code\n",
    "    qα_means[zip_code] = qα.mean().eval()\n",
    "    print(f'{zip_code}: {qα_means[zip_code]}')\n",
    "    \n",
    "open('means.py', 'w').write(repr(qα_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # retrodict μ_y of training data given inferred posterior parameter distributions\n",
    "# μ_y_posterior = ed.copy(μ_y, param_posteriors)\n",
    "# μ_y_posterior_preds = μ_y_posterior.eval( {fixed_effects: X_train, zip_codes_ph: zip_codes[train_index]} )\n",
    "\n",
    "# # group by zipcode, compute empirical mean `logerror` in training data\n",
    "# zip_codes_df = pd.DataFrame(\n",
    "#     data={'logerror': y_train, 'predicted_mean_logerror': μ_y_posterior_preds}, \n",
    "#     index=zip_codes[train_index]\n",
    "# )\n",
    "# zip_codes_df = zip_codes_df\\\n",
    "#     .groupby(level=0)\\\n",
    "#     .mean()\\\n",
    "#     .clip(-.15, .15)\n",
    "\n",
    "zip_codes_df = pd.Series(qα_means)\n",
    "# ! add in global mean + offsets from random effects model\n",
    "# then do the rest of this cell\n",
    "\n",
    "# join observation counts of each distinct zipcode\n",
    "zip_code_obs_counts = pd.Series(zip_codes[train_index].value_counts(), name='obs_count')\n",
    "zip_codes_df = zip_codes_df.join(zip_code_obs_counts)\n",
    "\n",
    "# cut counts into three buckets based on size\n",
    "zip_codes_df['n_obs_color'] = pd.cut(zip_codes_df['obs_count'], bins=3, labels=['#377eb8', '#4daf4a', '#ff7f00'])\n",
    "\n",
    "zip_codes_df = zip_codes_df\\\n",
    "    .sort_values(by='obs_count')\\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "# plot shrinkage\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.xlim(0, 400)\n",
    "average_logerror = zip_codes_df['logerror'].mean()\n",
    "\n",
    "plt.scatter(zip_codes_df.index, zip_codes_df['logerror'], facecolors='none',\n",
    "            edgecolors=zip_codes_df['n_obs_color'], s=50, linewidth=1, \n",
    "            alpha=1, label='Empirical Mean Log-Error')\n",
    "plt.scatter(zip_codes_df.index, zip_codes_df['predicted_mean_logerror'], c=zip_codes_df['n_obs_color'], \n",
    "            s=100, alpha=.7, label='Predicted Mean Log-Error')\n",
    "plt.axhline(average_logerror, color='#A60628', linestyle='--', label='Empirical Global Mean Log-Error')\n",
    "plt.xlabel('Zip Code Index \\n(In Order of Increasing Observation Counts)', fontsize=14)\n",
    "plt.ylabel('Log-Error', fontsize=14)\n",
    "plt.title('Shrinkage in Mean Log-Error Estimates from Linear Regression with Random Effects', fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with random effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network(fixed_effects, λ=.001, input_dim=D):\n",
    "    dense = Dense(5, activation='tanh', kernel_regularizer=l2(λ))(fixed_effects)\n",
    "    output = Dense(1, activation='linear', name='output', kernel_regularizer=l2(λ))(dense)\n",
    "    return K.squeeze(output, axis=1)\n",
    "\n",
    "# model\n",
    "μ_y = α + α_random_effects + neural_network(fixed_effects)\n",
    "y = Normal(loc=μ_y, scale=tf.ones(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vars = {\n",
    "    β_fixed_effects: qβ_fixed_effects,\n",
    "    α: qα,\n",
    "    α_zip_code: qα_zip_code\n",
    "}\n",
    "\n",
    "sess.run(INIT_OP)\n",
    "inference = ed.KLqp(latent_vars, data={fixed_effects: X_train, zip_codes_ph: zip_codes[train_index], y: y_train})\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "inference.initialize(optimizer=optimizer)\n",
    "inference.run(n_samples=5, n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_posteriors = {\n",
    "    β_fixed_effects: qβ_fixed_effects.mean(),\n",
    "    α: qα.mean(),\n",
    "    α_zip_code: qα_zip_code.mean()\n",
    "}\n",
    "X_val_feed_dict = {\n",
    "    fixed_effects: X_val,\n",
    "    zip_codes_ph: zip_codes[val_index]\n",
    "}\n",
    "y_posterior = ed.copy(y, param_posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mean_absolute_error(y_posterior, X_val_feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Inspect residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_residuals(y_posterior, X_val_feed_dict, title='Neural Network with Random Effects Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# future work\n",
    "\n",
    "- varying slopes models\n",
    "    - we probably need tensorflow for this?\n",
    "    - covariance matrices\n",
    "- we could have a random effect on each bias term for each layer\n",
    "- flexibilities all around\n",
    "- pymc does missing value imputation it seems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references\n",
    "- http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/\n",
    "- https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
    "- http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html\n",
    "- chapter 14 of statistical rethinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
